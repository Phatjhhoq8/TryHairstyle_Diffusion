# Hoáº¡t Äá»“ Há»‡ Thá»‘ng Huáº¥n Luyá»‡n (Training Pipeline)

## I. Tá»•ng Quan Pipeline

Pipeline huáº¥n luyá»‡n gá»“m **4 Giai Ä‘oáº¡n** (Stages) cháº¡y tuáº§n tá»±, má»—i giai Ä‘oáº¡n phá»¥ thuá»™c Ä‘áº§u ra cá»§a giai Ä‘oáº¡n trÆ°á»›c.

```mermaid
flowchart LR
    S0["Stage 0\nChuáº©n Bá»‹ Dataset"]
    S1["Stage 1\nTexture Encoder"]
    S2["Stage 2\nUNet Inpainting"]
    S3["Stage 3\nEvaluate & Export"]

    S0 -->|"processed/"| S1
    S1 -->|"texture_encoder.safetensors"| S2
    S2 -->|"deep_hair_v1.safetensors"| S3
    S3 -->|"Deploy"| PROD["Production\nbackend/training/models/"]

    classDef stage fill:#4a9eff,color:white,stroke:#2668b5,stroke-width:2px;
    classDef prod fill:#2ecc71,color:white,stroke:#27ae60,stroke-width:3px;
    class S0,S1,S2,S3 stage;
    class PROD prod;
```

**File Ä‘iá»u phá»‘i:** `run_training_pipeline.sh` â€” cháº¡y láº§n lÆ°á»£t 4 lá»‡nh Python.

---

## II. Stage 0 â€” Chuáº©n Bá»‹ Dataset

**File:** `prepare_dataset_deephair.py`

### Má»¥c tiÃªu
Chuyá»ƒn Ä‘á»•i áº£nh raw + JSON labels (K-Hairstyle) thÃ nh cÃ¡c tensor/áº£nh sáºµn sÃ ng cho training.

### Hoáº¡t Ä‘á»“ chi tiáº¿t

```mermaid
flowchart TD
    subgraph INPUT["ðŸ“‚ Input"]
        IMG["áº¢nh JPG/PNG\n(K-Hairstyle)"]
        JSON["JSON Labels\n(polygon, color, curl,\nlength, bang...)"]
        DICT["mapping_dict.json\n(HÃ n â†’ Anh)"]
    end

    IMG --> WORKER["process_single_image()"]
    JSON --> WORKER
    DICT --> PROMPT["generate_text_prompt()"]
    JSON --> PROMPT

    WORKER --> STEP1["1. Äá»c JSON polygon\nâ†’ cv2.fillPoly()\nâ†’ Hair Mask"]
    STEP1 --> STEP2["2. Detect Face\n(YOLO)\nâ†’ Bbox lá»›n nháº¥t"]
    STEP2 --> STEP3["3. cv2.inpaint()\nâ†’ Bald Image\n(XÃ³a tÃ³c)"]
    STEP3 --> STEP4["4. Merge RGBA\nâ†’ Hair Only\n(TÃ³c + Alpha)"]
    STEP4 --> STEP5["5. AdaFace Embedding\nâ†’ Identity .npy\n(512-dim vector)"]
    STEP5 --> STEP6["6. Crop + Resize\nâ†’ Style Vector\n(224Ã—224 image)"]
    STEP6 --> STEP7["7. Sliding Window\nâ†’ Hair Patches\n(128Ã—128, ratioâ‰¥85%)"]
    PROMPT --> STEP8["8. Text Prompt\n(Tiáº¿ng Anh)"]

    subgraph OUTPUT["ðŸ“ processed/"]
        OUT1["bald_images/*.png"]
        OUT2["hair_only_images/*.png"]
        OUT3["hair_patches/*.png"]
        OUT4["style_vectors/*.png"]
        OUT5["identity_embeddings/*.npy"]
        OUT6["metadata.jsonl"]
    end

    STEP3 --> OUT1
    STEP4 --> OUT2
    STEP7 --> OUT3
    STEP6 --> OUT4
    STEP5 --> OUT5
    STEP8 --> OUT6

    classDef input fill:#ffeaa7,stroke:#fdcb6e,stroke-width:2px;
    classDef process fill:#74b9ff,stroke:#0984e3,stroke-width:1px;
    classDef output fill:#55efc4,stroke:#00b894,stroke-width:2px;
    class IMG,JSON,DICT input;
    class STEP1,STEP2,STEP3,STEP4,STEP5,STEP6,STEP7,STEP8,WORKER,PROMPT process;
    class OUT1,OUT2,OUT3,OUT4,OUT5,OUT6 output;
```

### Äa luá»“ng
`ProcessPoolExecutor` (tá»‘i Ä‘a 8 workers) xá»­ lÃ½ song song nhiá»u áº£nh, má»—i worker khá»Ÿi táº¡o riÃªng YOLO detector + AdaFace embedder.

---

## III. Stage 1 â€” Hair Texture Encoder

**File:** `models/texture_encoder.py`

### Má»¥c tiÃªu
Huáº¥n luyá»‡n máº¡ng **ResNet50** Ä‘á»ƒ "hiá»ƒu" texture tÃ³c: xoÄƒn/tháº³ng, dÃ y/má»ng, lá»n/sá»£i.

### Kiáº¿n trÃºc Model

```mermaid
flowchart LR
    INPUT["Hair Patch\n128Ã—128Ã—3"]
    BB["ResNet50\nBackbone\n(ImageNet pretrained)"]
    EMBED["Embedding\n2048-dim"]

    INPUT --> BB --> EMBED

    EMBED --> PROJ["Projection Head\nMLP 2-layer\nâ†’ 128-dim"]
    EMBED --> CLS1["Curl Classifier\nDropout + Linear\nâ†’ 4 classes"]
    EMBED --> CLS2["Volume Classifier\nDropout + Linear\nâ†’ 3 classes"]

    classDef model fill:#a29bfe,color:white,stroke:#6c5ce7,stroke-width:2px;
    classDef head fill:#fd79a8,color:white,stroke:#e84393,stroke-width:1px;
    class BB model;
    class PROJ,CLS1,CLS2 head;
```

### QuÃ¡ trÃ¬nh Training

```mermaid
flowchart TD
    DATA["HairTextureDataset\n(Patches 128Ã—128\n+ nhÃ£n curl/volume\ntá»« metadata)"]

    DATA --> FWD["Forward Pass\nResNet50 â†’ embed, proj, cls"]
    FWD --> L1["Loss 1: CrossEntropyLoss\n(curl_logits vs curl_label)\n(volume_logits vs volume_label)"]
    FWD --> L2["Loss 2: SupConLoss\n(Supervised Contrastive)\nKÃ©o patches cÃ¹ng nhÃ£n láº¡i gáº§n\nÄáº©y patches khÃ¡c nhÃ£n ra xa"]

    L1 --> TOTAL["total_loss = L1 + 0.5 Ã— L2"]
    L2 --> TOTAL
    TOTAL --> BACK["Backprop + AdamW\n(lr=5e-4)"]
    BACK --> CKPT["Checkpoint má»—i 500 steps\nâ†’ stage1_step_N.safetensors"]

    classDef loss fill:#e17055,color:white,stroke:#d63031,stroke-width:1px;
    class L1,L2,TOTAL loss;
```

### NhÃ£n phÃ¢n loáº¡i (tá»« text_prompt metadata)

| Curl (4 classes) | Volume (3 classes) |
|---|---|
| 0 = straight (tháº³ng) | 0 = low (Ã­t) |
| 1 = wavy (vá»ƒnh) | 1 = normal (bÃ¬nh thÆ°á»ng) |
| 2 = curly (xoÄƒn) | 2 = high (nhiá»u) |
| 3 = tightly curly (xoÄƒn tÃ­t) | |

---

## IV. Stage 2 â€” Mask-Conditioned Hair Inpainting (Core)

**Files:** `train_stage2.py`, `models/stage2_unet.py`

### Má»¥c tiÃªu
Fine-tune UNet SDXL Ä‘á»ƒ váº½ tÃ³c má»›i vÃ o vÃ¹ng mask, giá»¯ nguyÃªn khuÃ´n máº·t.

### Kiáº¿n trÃºc UNet 9-Channel

```mermaid
flowchart TD
    subgraph ENCODE["VAE Encode (Frozen, fp16)"]
        GT["Ground Truth Image"] --> VAE1["VAE\nEncoder"] --> LAT["Latents\n4 channels"]
        BALD["Bald Image\n(ÄÃ£ xÃ³a tÃ³c)"] --> VAE2["VAE\nEncoder"] --> BLAT["Bald Latents\n4 channels"]
    end

    MASK["Hair Mask\n(Downsampled)\n1 channel"]

    LAT --> NOISE["Add Noise\n(DDPMScheduler)"]
    NOISE --> CONCAT

    subgraph CONCAT["Concat â†’ 9 Channels"]
        direction LR
        C1["Noisy Latents (4ch)"]
        C2["Mask (1ch)"]
        C3["Bald Latents (4ch)"]
    end

    BLAT --> CONCAT
    MASK --> CONCAT

    CONCAT --> UNET["SDXL UNet\n(9-ch conv_in patched)\nGradient Checkpointing"]

    subgraph COND["Cross-Attention Conditioning"]
        TEXT["Text Prompt\nâ†’ CLIP Encode\nâ†’ (77, 2048)"]
        STYLE["Style Embed\nâ†’ Linear+LayerNorm\nâ†’ (1, 2048)"]
        ID["Identity Embed\nâ†’ Linear+GELU+Linear\nâ†’ (1, 2048)"]
    end

    TEXT --> CAT_COND["Concat Sequence"]
    STYLE --> CAT_COND
    ID --> CAT_COND
    CAT_COND -->|"encoder_hidden_states"| UNET

    UNET --> PRED["Noise Prediction"]

    classDef vae fill:#00cec9,color:white,stroke:#00b894;
    classDef unet fill:#6c5ce7,color:white,stroke:#5f3dc4,stroke-width:3px;
    classDef cond fill:#fdcb6e,stroke:#f39c12;
    class VAE1,VAE2 vae;
    class UNET unet;
    class TEXT,STYLE,ID,CAT_COND cond;
```

### Luá»“ng Training 1 Step

```mermaid
flowchart TD
    BATCH["DataLoader\n(batch_size=1)"]
    BATCH --> VAE_ENC["VAE Encode\n(frozen, fp16)\nGT â†’ latents\nBald â†’ bald_latents"]

    VAE_ENC --> ADD_NOISE["DDPMScheduler\nadd_noise(latents, noise,\nrandom timestep 0~999)"]

    ADD_NOISE --> FWD["UNet Forward\n(AMP fp16)\n9-ch input + conditions\nâ†’ noise_pred"]

    FWD --> LOSS1["Loss 1: MaskAwareLoss\n|(noise_pred - noise)Â² Ã— mask|\nChá»‰ vÃ¹ng tÃ³c, khÃ³a máº·t"]

    FWD -->|"Má»—i 10 steps"| LOSS2["Loss 2: TextureConsistencyLoss\nDecode latents â†’ RGB\nVGG16 Gram Matrix\nSo sÃ¡nh texture"]

    LOSS1 --> TOTAL["total = L1 + 0.01Ã—L2"]
    LOSS2 --> TOTAL

    TOTAL --> BACK["AMP Backprop\n+ Gradient Clipping (norm=1.0)\n+ AdamW (lr=1e-5)"]

    BACK --> SAVE["Checkpoint:\n- Má»—i 500 steps\n- Cuá»‘i má»—i epoch\nâ†’ .safetensors"]

    classDef loss fill:#e17055,color:white;
    classDef save fill:#2ecc71,color:white;
    class LOSS1,LOSS2,TOTAL loss;
    class SAVE save;
```

### VRAM Budget (GPU 12GB â€” RTX 3060)

| Component | VRAM | Ghi chÃº |
|---|---|---|
| UNet (fp16) | ~5 GB | Gradient checkpointing + xformers ON |
| VAE (fp16, frozen) | ~0.5 GB | Chá»‰ encode, enable_slicing() |
| Text Encoders | **0 GB** | Encode xong â†’ giáº£i phÃ³ng |
| Optimizer states | ~2.5 GB | 8-bit AdamW (bitsandbytes) |
| Activations + Gradients | ~3-4 GB | AMP fp16 + xformers + grad accumulation |
| **Tá»•ng Æ°á»›c tÃ­nh** | **~10-11 GB** | Fit trong 12GB âœ… |

> **Ká»¹ thuáº­t tá»‘i Æ°u Ä‘Ã£ Ã¡p dá»¥ng:**
> - xformers memory-efficient attention
> - 8-bit AdamW optimizer (bitsandbytes)
> - VAE slicing (encode/decode tá»«ng slice)
> - Gradient Accumulation (4 steps)
> - Texture Loss giáº£m táº§n suáº¥t (má»—i 50 steps)

### Pre-encode Workflow (Text Prompts)

```mermaid
sequenceDiagram
    participant TE as Text Encoders (CLIP)
    participant DS as Dataset
    participant GPU as GPU VRAM

    Note over TE,GPU: Giai Ä‘oáº¡n 1: Encode (Chá»‰ 1 láº§n)
    DS->>TE: Load metadata.jsonl
    TE->>GPU: Load CLIPTextModel + CLIPTextModelWithProjection (~3GB)
    loop Má»—i sample
        TE->>TE: tokenize(prompt) â†’ encode â†’ (77, 2048) + (1280,)
        TE->>DS: Save cache .pt vÃ o processed/prompt_embeddings/
    end
    TE->>GPU: Giáº£i phÃ³ng Text Encoders (del + empty_cache)

    Note over DS,GPU: Giai Ä‘oáº¡n 2: Training (Nhiá»u epochs)
    DS->>DS: Load .pt cache trá»±c tiáº¿p (khÃ´ng cáº§n Text Encoders)
    DS->>GPU: Chá»‰ UNet + VAE trÃªn VRAM
```

---

## V. Stage 3 â€” Evaluate & Export

**Files:** `evaluate.py`, `export_model.py`

### Hoáº¡t Ä‘á»“

```mermaid
flowchart TD
    FIND["TÃ¬m checkpoint má»›i nháº¥t\n(*stage2*.safetensors)"]
    FIND --> LOAD["Load state_dict\n(safetensors)"]
    LOAD --> VALID{"Checkpoint há»£p lá»‡?\n(>10 params, cÃ³ conv_in)"}

    VALID -->|"KhÃ´ng"| FAIL["âŒ Model chÆ°a Ä‘á»§\ncháº¥t lÆ°á»£ng"]
    VALID -->|"CÃ³"| EVAL["Cháº¡y Evaluation\ntrÃªn 5 validation samples"]

    EVAL --> LPIPS["LPIPS Score\n(VGG Perceptual)\nCÃ ng tháº¥p cÃ ng tá»‘t"]
    EVAL --> PSNR["Masked PSNR\nChá»‰ tÃ­nh vÃ¹ng tÃ³c\nCÃ ng cao cÃ ng tá»‘t"]

    LPIPS --> DECIDE{"Äáº¡t threshold?"}
    PSNR --> DECIDE
    DECIDE -->|"Äáº¡t"| COPY["shutil.copy2()\ncheckpoint â†’ models/\ndeep_hair_v1.safetensors"]
    DECIDE -->|"KhÃ´ng Ä‘áº¡t"| FAIL

    COPY --> DONE["âœ… Deploy xong!\nWeb App load model má»›i"]

    classDef pass fill:#2ecc71,color:white;
    classDef fail fill:#e74c3c,color:white;
    class DONE pass;
    class FAIL fail;
```

### Metrics giáº£i thÃ­ch

| Metric | CÃ´ng thá»©c | Ã nghÄ©a |
|---|---|---|
| **LPIPS** | VGG features distance (crop vÃ¹ng tÃ³c) | Äo sá»± khÃ¡c biá»‡t thá»‹ giÃ¡c giá»¯a áº£nh sinh ra vÃ  áº£nh gá»‘c (â‰¤0.20 = tá»‘t) |
| **Masked PSNR** | 10Ã—logâ‚â‚€(1/MSE) chá»‰ trong mask | Äo cháº¥t lÆ°á»£ng pixel vÃ¹ng tÃ³c (â‰¥25 = tá»‘t) |

---

## VI. Cáº¥u TrÃºc File Há»‡ Thá»‘ng

```
backend/training/
â”œâ”€â”€ run_training_pipeline.sh      â† Script cháº¡y toÃ n bá»™ 4 stages
â”œâ”€â”€ prepare_dataset_deephair.py   â† Stage 0: Táº¡o dataset
â”œâ”€â”€ train_stage2.py               â† Stage 2: Training UNet chÃ­nh
â”œâ”€â”€ evaluate.py                   â† Metrics (LPIPS, PSNR)
â”œâ”€â”€ export_model.py               â† Stage 3: Validate + Deploy
â”œâ”€â”€ training_face.py              â† Face processing pipeline
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ texture_encoder.py        â† Stage 1: ResNet50 Texture Encoder
â”‚   â”œâ”€â”€ stage2_unet.py            â† UNet 9-channel + IP-Adapter Injector
â”‚   â””â”€â”€ losses.py                 â† Loss functions (SupCon, MaskAware, Identity, Texture)
â”œâ”€â”€ data_processing/
â”‚   â”œâ”€â”€ mapping_dict.json         â† Báº£ng dá»‹ch HÃ n â†’ Anh (K-Hairstyle)
â”‚   â”œâ”€â”€ auto_translate.py         â† Tool dá»‹ch tá»± Ä‘á»™ng
â”‚   â””â”€â”€ normalize_khairstyle.py   â† Chuáº©n hÃ³a dataset
â”œâ”€â”€ processed/                    â† Output cá»§a Stage 0 (tá»± táº¡o khi cháº¡y)
â”‚   â”œâ”€â”€ bald_images/
â”‚   â”œâ”€â”€ hair_only_images/
â”‚   â”œâ”€â”€ hair_patches/
â”‚   â”œâ”€â”€ style_vectors/
â”‚   â”œâ”€â”€ identity_embeddings/
â”‚   â”œâ”€â”€ prompt_embeddings/        â† Cache CLIP embeddings
â”‚   â””â”€â”€ metadata.jsonl
â””â”€â”€ checkpoints/                  â† Weights lÆ°u trong quÃ¡ trÃ¬nh training
    â”œâ”€â”€ stage1_step_500.safetensors
    â”œâ”€â”€ stage2_step_500.safetensors
    â”œâ”€â”€ stage2_epoch_1.safetensors
    â””â”€â”€ deep_hair_v1_latest.safetensors
```

---

## VII. CÃ¡ch Cháº¡y

```bash
# TrÃªn WSL (Linux)
cd /mnt/c/Users/Admin/Desktop/TryHairStyle
source venv_wsl/bin/activate

# Cháº¡y toÃ n bá»™ pipeline tá»± Ä‘á»™ng (4 stages)
bash backend/training/run_training_pipeline.sh

# Hoáº·c cháº¡y tá»«ng stage riÃªng:
python backend/training/prepare_dataset_deephair.py   # Stage 0
python backend/training/models/texture_encoder.py      # Stage 1
python backend/training/train_stage2.py                # Stage 2
python backend/training/export_model.py                # Stage 3
```

### YÃªu cáº§u trÆ°á»›c khi cháº¡y

| YÃªu cáº§u | ÄÆ°á»ng dáº«n |
|---|---|
| Dataset K-Hairstyle (images) | `backend/data/dataset/khairstyle/training/images/` |
| Dataset K-Hairstyle (labels) | `backend/data/dataset/khairstyle/training/labels/` |
| SDXL Inpainting Model | `backend/models/stable-diffusion/sd_xl_inpainting/` |
| GPU VRAM | â‰¥ 12 GB (RTX 3060 trá»Ÿ lÃªn, Ä‘Ã£ tá»‘i Æ°u) |

---

## VIII. Má»‘i Quan Há»‡ Training â†” Production

```mermaid
flowchart LR
    subgraph TRAINING["ðŸŽ“ Training Pipeline"]
        direction TB
        T0["prepare_dataset"] --> T1["texture_encoder"]
        T1 --> T2["train_stage2"]
        T2 --> T3["export_model"]
    end

    subgraph PRODUCTION["ðŸš€ Production Pipeline"]
        direction TB
        P1["User Upload\nTarget + Reference"]
        P2["SegFormer\nFace Mask"]
        P3["InsightFace\nID Embedding"]
        P4["SDXL UNet\n+ IP-Adapter\n+ ControlNet"]
        P5["VAE Decode\nâ†’ Output Image"]
        P1 --> P2 --> P4
        P1 --> P3 --> P4
        P4 --> P5
    end

    T3 -->|"deep_hair_v1.safetensors\n(Weights Ä‘Ã£ train)"| P4

    classDef training fill:#74b9ff,color:white,stroke:#0984e3;
    classDef prod fill:#55efc4,stroke:#00b894;
    class T0,T1,T2,T3 training;
    class P1,P2,P3,P4,P5 prod;
```

Model sau khi train xong Ä‘Æ°á»£c copy vÃ o thÆ° má»¥c `backend/training/models/` vÃ  Web App (FastAPI) sáº½ load weights má»›i khi khá»Ÿi Ä‘á»™ng láº¡i server.
